import os
import sys
import logging
import json
from typing import Tuple, Dict, List, Optional
import numpy as np
import torch
import torchaudio
from faster_whisper import WhisperModel

# TransformersëŠ” ì„ íƒì ìœ¼ë¡œ import
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ì‚¬ìš©")
from dataclasses import dataclass

# --- ë¡œê¹… ì„¤ì • ---
# ë¡œê·¸ë¥¼ íŒŒì¼ê³¼ ì½˜ì†”ì— ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('core_analysis.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# --- ë¶„ì„ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜ ---
@dataclass
class VoiceToneAnalysis:
    """ìŒì„± í†¤ ë¶„ì„ ê²°ê³¼"""
    pitch_variation: float
    speaking_speed: float
    volume_consistency: float
    warmth_score: float
    enthusiasm_level: float
    politeness_level: float
    confidence_level: float
    volume_strength: float

@dataclass
class WordChoiceAnalysis:
    """ë‹¨ì–´ ì„ íƒ ë¶„ì„ ê²°ê³¼"""
    positive_words: List[str]
    negative_words: List[str]
    polite_phrases: List[str]
    empathy_indicators: List[str]
    enthusiasm_indicators: List[str]
    politeness_score: float
    empathy_score: float
    enthusiasm_score: float
    valence_score: float

@dataclass
class DatingEmpathyScore:
    """ì¢…í•© ì ìˆ˜ ë° ë¶„ì„ ê²°ê³¼"""
    total_score: float
    voice_tone_score: float
    word_choice_score: float
    overall_mood: str
    voice_analysis: VoiceToneAnalysis
    word_analysis: WordChoiceAnalysis
    evidence: Dict[str, str]
    recommendations: List[str]
    voice_details: Dict[str, float] = None
    word_details: Dict[str, float] = None
    weights: Dict[str, float] = None
    positive_words: List[str] = None
    negative_words: List[str] = None

# --- ìƒìˆ˜ ë° ì„¤ì •ê°’ ---
TARGET_SR = 16000 # ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë ˆì´íŠ¸ (16kHz)

# ë‹¨ì–´ ë¶„ì„ì— ì‚¬ìš©ë  ì‚¬ì „
POSITIVE_DATING_WORDS = { "ì¢‹ì•„": 0.8, "ê°ì‚¬": 0.9, "í–‰ë³µ": 0.9, "ì‚¬ë‘": 1.0, "ì˜ˆì˜": 0.9, "ë©‹ìˆ": 0.8, "ìµœê³ ": 0.9, "íŠ¹ë³„": 0.8, "ê¸°ì˜": 0.8, "ê´œì°®": 0.6, "ë§ì•„": 0.7, "ì •ë§": 0.7 }
NEGATIVE_DATING_WORDS = { "ì‹«ì–´": -0.8, "í™”ë‚˜": -0.9, "ì§œì¦": -0.8, "ê·€ì°®": -0.7, "í˜ë“¤": -0.6, "ì–´ë ¤ì›Œ": -0.5, "ì•„í”„": -0.6, "ë¶ˆì•ˆ": -0.7, "ìŠ¬í¼": -0.7, "ë¬´ì„œì›Œ": -0.6 }
POLITE_PHRASES = [ "ê°ì‚¬í•©ë‹ˆë‹¤", "ê³ ë§™ìŠµë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤", "ë¶€íƒë“œë¦½ë‹ˆë‹¤", "~í•´ì£¼ì„¸ìš”", "~ì‹œê² ì–´ìš”" ]
EMPATHY_INDICATORS = [ "ê·¸ë ‡êµ¬ë‚˜", "ì´í•´í•´", "ê³µê°í•´", "í˜ë“¤ì—ˆê² ë‹¤", "ê´œì°®ì„ ê±°ì•¼", "ì–´ë–»ê²Œ ìƒê°í•´" ]
ENTHUSIASM_INDICATORS = [ "ì™€!", "ëŒ€ë°•!", "ì§„ì§œ?", "ì •ë§?", "ìµœê³ ì•¼!", "ë©‹ì ¸!" ]

# AI ëª¨ë¸ ID
AUDIO_EMO_MODEL_ID = "superb/wav2vec2-base-superb-er"
NLI_MODEL_ID = "cardiffnlp/twitter-roberta-base-emotion-multilingual-latest"  # ë” í˜¸í™˜ì„± ì¢‹ì€ ê°ì • ë¶„ì„ ëª¨ë¸
ASR_MODEL_NAME = "large-v3" # Whisper ëª¨ë¸

# ê°ì • ë ˆì´ë¸” ë§¤í•‘ (ì˜ì–´ -> í•œêµ­ì–´)
AUDIO_LABEL_MAP = { "angry": "ë¶„ë…¸", "calm": "ì¤‘ë¦½", "disgust": "í˜ì˜¤", "fearful": "ë‘ë ¤ì›€", "happy": "ê¸°ì¨", "neutral": "ì¤‘ë¦½", "sad": "ìŠ¬í””", "surprise": "ë†€ëŒ" }
KOREAN_EMOTION_LABELS = [ "ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ì¤‘ë¦½", "ë‘ë ¤ì›€", "í˜ì˜¤", "ë†€ëŒ", "ì§œì¦", "ê·€ì°®ìŒ", "ì„œìš´í•¨", "ì–µìš¸í•¨", "ë¶ˆì•ˆ", "ë¬´ì‹¬í•¨" ]

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ (ì´ˆê¸°ì—ëŠ” ë¹„ì–´ìˆìŒ)
_asr_model: Optional[WhisperModel] = None
_nli_tokenizer = None  # Optional[AutoTokenizer] = None
_nli_model = None  # Optional[AutoModelForSequenceClassification] = None
_audio_clf = None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

def preload_models():
    """
    ë¶„ì„ì— í•„ìš”í•œ ëª¨ë“  AI ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
    GPU ì‚¬ìš©ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©°, GPUê°€ ì—†ì„ ê²½ìš° CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ìŒì„± ë¶„ì„ ëª¨ë“ˆì´ ì‹¤íŒ¨í•´ë„ ASR ëª¨ë¸ì€ ë¡œë“œë©ë‹ˆë‹¤.
    """
    global _asr_model, _nli_tokenizer, _nli_model, _audio_clf
    
    if _asr_model is not None:
        logger.info("Models are already loaded.")
        return

    logger.info("Starting to preload models...")
    
    if torch.cuda.is_available():
        device = "cuda"
        # ìµœì‹  GPUì—ì„œëŠ” float16ì„ ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ
        gpu_capability = torch.cuda.get_device_capability(0)[0]
        asr_compute_type = "float16" if gpu_capability >= 7 else "int8"
        
        logger.info(f"ğŸ® GPU ê°ì§€ë¨!")
        logger.info(f"ğŸ® GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        logger.info(f"ğŸ® GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ® GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        logger.info(f"ğŸ® GPU ì„±ëŠ¥ ë“±ê¸‰: {gpu_capability}.x")
        logger.info(f"ğŸ® CUDA ë²„ì „: {torch.version.cuda}")
        logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}, ASR ì—°ì‚° íƒ€ì…: {asr_compute_type}")
    else:
        device = "cpu"
        asr_compute_type = "int8" # CPUì—ì„œëŠ” ê°€ë²¼ìš´ íƒ€ì… ì‚¬ìš©
        logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ëŒ€ì²´ (ì†ë„ê°€ í˜„ì €íˆ ëŠë ¤ì§‘ë‹ˆë‹¤)")
        logger.info(f"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}, ASR ì—°ì‚° íƒ€ì…: {asr_compute_type}")

    # 1. ìŒì„± ì¸ì‹ ëª¨ë¸ (Whisper) - í•„ìˆ˜ ëª¨ë¸
    try:
        _asr_model = WhisperModel(ASR_MODEL_NAME, device=device, compute_type=asr_compute_type)
        logger.info("âœ… ASR ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ ASR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        _asr_model = None
    
    # 2. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ëª¨ë¸ (í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”)
    logger.info("Using keyword-based emotion analysis for better compatibility")
    _nli_tokenizer = None
    _nli_model = None
    
    # 3. ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ (wav2vec2) - ì„ íƒì  ëª¨ë¸
    _audio_clf = None
    if TRANSFORMERS_AVAILABLE:
        try:
            device_id = 0 if device == "cuda" else -1
            _audio_clf = pipeline("audio-classification", model=AUDIO_EMO_MODEL_ID, device=device_id)
            logger.info("âœ… ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´): {e}")
            _audio_clf = None
    else:
        logger.info("Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ìŒì„± ê°ì • ë¶„ì„ ë¹„í™œì„±í™” (í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´)")
        _audio_clf = None
    
    # ìµœì¢… ìƒíƒœ ë³´ê³ 
    if _asr_model is not None:
        logger.info("âœ… ASR ëª¨ë¸ ë¡œë“œë¨ - STT ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ ASR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - STT ê¸°ëŠ¥ ì œí•œë¨")
    
    if _audio_clf is not None:
        logger.info("âœ… ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œë¨")
    else:
        logger.info("â„¹ï¸ ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ ë¹„í™œì„±í™” - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ì‚¬ìš©")
    
    logger.info("ğŸ¯ ëª¨ë¸ ë¡œë”© ì™„ë£Œ - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥")


def analyze_voice_tone(audio_array: np.ndarray, sr: int) -> VoiceToneAnalysis:
    """ìŒì„± í†¤ì˜ ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    # ì‹¤ì œ ë¶„ì„ ë¡œì§ì€ ë³µì¡í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” í•µì‹¬ ê°œë…ì„ ë³´ì—¬ì£¼ëŠ” ê°„ì†Œí™”ëœ ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    try:
        rms = np.sqrt(np.mean(np.square(audio_array)))
        # FFTë¥¼ ì´ìš©í•œ ìŒë†’ì´ ë³€í™”ëŸ‰ ì¶”ì •
        fft_abs = np.abs(np.fft.fft(audio_array))
        pitch_variation = min(1.0, np.std(fft_abs) / 5000)
        # ì˜¤ë””ì˜¤ë¥¼ 20ê°œë¡œ ë‚˜ëˆ„ì–´ ë³¼ë¥¨ ì¼ê´€ì„± ê³„ì‚°
        chunk_volumes = [np.sqrt(np.mean(np.square(c))) for c in np.array_split(audio_array, 20) if c.size > 0]
        volume_consistency = max(0.0, 1.0 - np.std(chunk_volumes) / 0.1) if len(chunk_volumes) > 1 else 0.5
        
        return VoiceToneAnalysis(
            pitch_variation=pitch_variation,
            speaking_speed=3.0, # ì‹¤ì œë¡œëŠ” STT ê²°ê³¼ì™€ ì‹œê°„ì„ ë¹„êµí•´ì•¼ í•¨ (ì—¬ê¸°ì„œëŠ” ê³ ì •ê°’)
            volume_consistency=volume_consistency,
            warmth_score=min(1.0, rms * 5 + 0.3),
            enthusiasm_level=min(1.0, pitch_variation * 1.5),
            politeness_level=(volume_consistency + (1.0 - pitch_variation)) / 2,
            confidence_level=min(1.0, rms * 7),
            volume_strength=min(1.0, rms * 10)
        )
    except Exception as e:
        logger.error(f"Voice tone analysis failed: {e}")
        return VoiceToneAnalysis(0.5, 3.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)

def analyze_word_choice(text: str, elapsed_sec: float = 0.0) -> WordChoiceAnalysis:
    """í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ì„ íƒì„ ë§¥ë½ê³¼ ì‹œê°„ì„ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤."""
    positive_words = [w for w, s in POSITIVE_DATING_WORDS.items() if w in text]
    negative_words = [w for w, s in NEGATIVE_DATING_WORDS.items() if w in text]
    polite_phrases = [p for p in POLITE_PHRASES if p in text]
    empathy_indicators = [e for e in EMPATHY_INDICATORS if e in text]
    enthusiasm_indicators = [e for e in ENTHUSIASM_INDICATORS if e in text]

    pos_score = sum(POSITIVE_DATING_WORDS.get(w, 0) for w in positive_words)
    neg_score = sum(abs(NEGATIVE_DATING_WORDS.get(w, 0)) for w in negative_words)

    # ëŒ€í™” ì´ˆë°˜(30ì´ˆ ì´ë‚´)ì— "ì‚¬ë‘"ê³¼ ê°™ì€ ê°•í•œ í‘œí˜„ì€ ì˜¤íˆë ¤ ê°ì 
    if elapsed_sec < 30 and "ì‚¬ë‘" in text:
        pos_score *= 0.6
        neg_score += 0.3

    # ê¸ì • ì ìˆ˜ì™€ ë¶€ì • ì ìˆ˜ë¥¼ ì¢…í•©í•˜ì—¬ 0~1 ì‚¬ì´ì˜ ì •ì„œ ì ìˆ˜(valence) ê³„ì‚°
    valence_score = max(0.0, min(1.0, 0.5 + (pos_score - neg_score) / 5.0))

    return WordChoiceAnalysis(
        positive_words=positive_words, negative_words=negative_words,
        polite_phrases=polite_phrases, empathy_indicators=empathy_indicators,
        enthusiasm_indicators=enthusiasm_indicators,
        politeness_score=min(1.0, len(polite_phrases) / 2.0),
        empathy_score=min(1.0, len(empathy_indicators) / 2.0),
        enthusiasm_score=min(1.0, len(enthusiasm_indicators) / 2.0),
        valence_score=valence_score
    )

def calculate_dating_empathy_score(voice_analysis: VoiceToneAnalysis, word_analysis: WordChoiceAnalysis, emotion_scores: List[Tuple[str, float]]) -> DatingEmpathyScore:
    """ìŒì„±, ë‹¨ì–´, ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # ê° ìš”ì†Œì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ì ìˆ˜ ê³„ì‚°
    voice_tone_score = (voice_analysis.warmth_score * 0.25 + voice_analysis.politeness_level * 0.25 + voice_analysis.volume_consistency * 0.15 + voice_analysis.enthusiasm_level * 0.15 + voice_analysis.confidence_level * 0.20) * 100
    word_choice_score = (word_analysis.politeness_score * 0.30 + word_analysis.empathy_score * 0.30 + word_analysis.enthusiasm_score * 0.15 + word_analysis.valence_score * 0.25) * 100

    emotion_dict = dict(emotion_scores)
    positive_emotions = ["ê¸°ì¨", "ì¤‘ë¦½", "ë†€ëŒ"]
    negative_emotions = ["ë¶„ë…¸", "ìŠ¬í””", "ë‘ë ¤ì›€", "í˜ì˜¤", "ì§œì¦"]
    pos_emo_score = sum(emotion_dict.get(e, 0) for e in positive_emotions)
    neg_emo_score = sum(emotion_dict.get(e, 0) for e in negative_emotions)
    emotion_score = max(0, min(100, (pos_emo_score - neg_emo_score * 0.5) * 100))

    # ìµœì¢… ì ìˆ˜ (ìŒì„± 40%, ë‹¨ì–´ 40%, ê°ì • 20%)
    total_score = (voice_tone_score * 0.4 + word_choice_score * 0.4 + emotion_score * 0.2)
    
    # ì ìˆ˜ì— ë”°ë¥¸ ë¶„ìœ„ê¸° ë° ì¡°ì–¸ ìƒì„±
    mood = "ë³´í†µ"
    if total_score >= 85: mood = "ë§¤ìš° ì¢‹ìŒ"
    elif total_score >= 70: mood = "ì¢‹ìŒ"
    elif total_score >= 30: mood = "ë‚˜ì¨"
    else: mood = "ë§¤ìš° ë‚˜ì¨"
    recommendations = ["ì§€ê¸ˆì²˜ëŸ¼ë§Œ í•˜ì‹œë©´ ë¼ìš”! ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤."] if total_score > 70 else ["ì¡°ê¸ˆ ë” ìì‹ ê° ìˆê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ë§í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"]

    return DatingEmpathyScore(
        total_score=total_score, voice_tone_score=voice_tone_score, word_choice_score=word_choice_score,
        overall_mood=mood, voice_analysis=voice_analysis, word_analysis=word_analysis,
        evidence={"summary": "Analysis complete."}, recommendations=recommendations,
        voice_details=vars(voice_analysis), word_details=vars(word_analysis),
        weights={"voice": 0.4, "word": 0.4, "emotion": 0.2},
        positive_words=word_analysis.positive_words, negative_words=word_analysis.negative_words
    )

def transcribe_korean(audio_array: np.ndarray) -> str:
    """ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if _asr_model is None:
        logger.warning("ASR model is not loaded. Cannot perform transcription.")
        return ""
    try:
        segments, _ = _asr_model.transcribe(audio_array, language="ko", beam_size=5)
        return " ".join(seg.text for seg in segments).strip()
    except Exception as e:
        logger.error(f"Transcription failed: {e}")
        return ""

def classify_emotion_audio(audio_array: np.ndarray) -> List[Tuple[str, float]]:
    """ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ ì§ì ‘ ê°ì •ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    if _audio_clf is None:
        logger.info("Audio emotion classifier not loaded, using neutral emotion")
        return [("ì¤‘ë¦½", 1.0)]
    try:
        result = _audio_clf({"array": audio_array, "sampling_rate": TARGET_SR}, top_k=None)
        # ê²°ê³¼ë¥¼ í•œêµ­ì–´ ë ˆì´ë¸”ë¡œ ë³€í™˜í•˜ê³  ì ìˆ˜ í•©ì‚°
        agg = {}
        for item in result:
            ko_label = AUDIO_LABEL_MAP.get(item["label"], item["label"])
            agg[ko_label] = agg.get(ko_label, 0.0) + item["score"]
        return sorted(agg.items(), key=lambda x: x[1], reverse=True)
    except Exception as e:
        logger.error(f"Audio emotion classification failed: {e}")
        return [("ì¤‘ë¦½", 1.0)]

def classify_emotion_ko_zeroshot(text: str) -> List[Tuple[str, float]]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê°ì •ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    if not text:
        return [("ì¤‘ë¦½", 1.0)]
    
    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©
    if _nli_tokenizer is None or _nli_model is None:
        return classify_emotion_by_keywords(text)
    
    scores = []
    try:
        # ìƒˆë¡œìš´ ëª¨ë¸ì˜ ë ˆì´ë¸” êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        device = _nli_model.device
        enc = _nli_tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
        
        with torch.no_grad():
            logits = _nli_model(**enc).logits
            probs = torch.softmax(logits, dim=-1).squeeze(0)
        
        # ëª¨ë¸ì˜ ë ˆì´ë¸”ì— ë”°ë¼ ê°ì • ë§¤í•‘
        labels = ["ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ì¤‘ë¦½"]
        for i, label in enumerate(labels):
            if i < len(probs):
                scores.append((label, float(probs[i])))
        
        if not scores:
            scores = [("ì¤‘ë¦½", 1.0)]
            
        return sorted(scores, key=lambda x: x[1], reverse=True)
    except Exception as e:
        logger.error(f"Text emotion classification failed: {e}")
        return classify_emotion_by_keywords(text)

def classify_emotion_by_keywords(text: str) -> List[Tuple[str, float]]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
    if not text:
        return [("ì¤‘ë¦½", 1.0)]
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ì ìˆ˜ ê³„ì‚°
    positive_score = sum(1 for word in POSITIVE_DATING_WORDS if word in text)
    negative_score = sum(1 for word in NEGATIVE_DATING_WORDS if word in text)
    enthusiasm_score = sum(1 for word in ENTHUSIASM_INDICATORS if word in text)
    
    if enthusiasm_score > 0 or positive_score > negative_score:
        return [("ê¸°ì¨", 0.8), ("ì¤‘ë¦½", 0.2)]
    elif negative_score > positive_score:
        return [("ìŠ¬í””", 0.7), ("ì¤‘ë¦½", 0.3)]
    else:
        return [("ì¤‘ë¦½", 1.0)]

def process_audio_simple(audio_array: np.ndarray) -> dict:
    """
    ì˜¤ë””ì˜¤ ë°°ì—´ì„ ì…ë ¥ë°›ì•„ ëª¨ë“  ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    ìŒì„± ë¶„ì„ ëª¨ë“ˆì´ ì‹¤íŒ¨í•´ë„ STTì™€ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ì€ ì‘ë™í•©ë‹ˆë‹¤.
    """
    result = {}
    try:
        # 1. ìŒì„± ì¸ì‹ (STT) - ASR ëª¨ë¸ì´ ì—†ì–´ë„ ê¸°ë³¸ê°’ ì²˜ë¦¬
        transcript = ""
        if _asr_model is not None:
            try:
                transcript = transcribe_korean(audio_array)
            except Exception as e:
                logger.warning(f"STT failed, using fallback: {e}")
                transcript = "ìŒì„± ì¸ì‹ ì‹¤íŒ¨"
        else:
            logger.warning("ASR model not loaded, skipping STT")
            transcript = "ìŒì„± ì¸ì‹ ëª¨ë“ˆ ë¹„í™œì„±í™”"
        
        result['transcript'] = transcript
        
        # 2. ê°ì • ë¶„ì„ - ìŒì„± ë¶„ì„ ëª¨ë“ˆì´ ì‹¤íŒ¨í•´ë„ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
        final_emotion_scores = [("ì¤‘ë¦½", 1.0)]  # ê¸°ë³¸ê°’
        
        if transcript and transcript != "ìŒì„± ì¸ì‹ ì‹¤íŒ¨" and transcript != "ìŒì„± ì¸ì‹ ëª¨ë“ˆ ë¹„í™œì„±í™”":
            # í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ì‚¬ìš©
            final_emotion_scores = classify_emotion_by_keywords(transcript)
            logger.info(f"Using keyword-based emotion analysis: {final_emotion_scores}")
        else:
            # ìŒì„± ê°ì • ë¶„ì„ ì‹œë„ (ëª¨ë“ˆì´ ë¡œë“œëœ ê²½ìš°)
            if _audio_clf is not None:
                try:
                    audio_scores = classify_emotion_audio(audio_array)
                    final_emotion_scores = audio_scores
                    logger.info(f"Using audio-based emotion analysis: {audio_scores}")
                except Exception as e:
                    logger.warning(f"Audio emotion analysis failed, using neutral: {e}")
            else:
                logger.info("Audio emotion module not loaded, using neutral emotion")
        
        top_emotion, top_score = final_emotion_scores[0]
        result['emotion'] = str(top_emotion)
        result['emotion_score'] = float(top_score)
        
        # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚° - ìŒì„± ë¶„ì„ì´ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ê³„ì‚°
        elapsed_sec = len(audio_array) / TARGET_SR
        
        # ìŒì„± í†¤ ë¶„ì„ (ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©)
        try:
            voice_analysis = analyze_voice_tone(audio_array, TARGET_SR)
        except Exception as e:
            logger.warning(f"Voice tone analysis failed, using defaults: {e}")
            voice_analysis = VoiceToneAnalysis(0.5, 3.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)
        
        # ë‹¨ì–´ ì„ íƒ ë¶„ì„ (STT ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        if transcript and transcript not in ["ìŒì„± ì¸ì‹ ì‹¤íŒ¨", "ìŒì„± ì¸ì‹ ëª¨ë“ˆ ë¹„í™œì„±í™”"]:
            word_analysis = analyze_word_choice(transcript, elapsed_sec)
        else:
            logger.warning("No transcript available, using default word analysis")
            word_analysis = WordChoiceAnalysis([], [], [], [], [], 0.5, 0.5, 0.5, 0.5)
        
        dating_score = calculate_dating_empathy_score(voice_analysis, word_analysis, final_emotion_scores)
        
        # 4. ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€ (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜)
        result.update({
            'total_score': float(dating_score.total_score),
            'voice_tone_score': float(dating_score.voice_tone_score),
            'voice_details': {k: float(v) for k, v in dating_score.voice_details.items()},
            'word_choice_score': float(dating_score.word_choice_score),
            'word_details': {k: float(v) if isinstance(v, (int, float)) else v for k, v in dating_score.word_details.items()},
            'weights': {k: float(v) for k, v in dating_score.weights.items()},
            'positive_words': list(dating_score.positive_words),
            'negative_words': list(dating_score.negative_words)
        })

    except Exception as e:
        logger.critical(f"Critical error in audio processing pipeline: {e}", exc_info=True)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            'transcript': 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ', 'emotion': 'ì¤‘ë¦½', 'emotion_score': 1.0,
            'total_score': 50.0, 'voice_tone_score': 50.0, 'voice_details': {},
            'word_choice_score': 50.0, 'word_details': {}, 'weights': {},
            'positive_words': [], 'negative_words': []
        }
    return result

# --- ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ ---
if __name__ == "__main__":
    """
    ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°, ì•„ë˜ í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ ë™ì‘í•©ë‹ˆë‹¤.
    ëª¨ë¸ ë¡œë”©ê³¼ ê°„ë‹¨í•œ ê°€ìƒ ì˜¤ë””ì˜¤ ë°ì´í„° ë¶„ì„ì„ í†µí•´ ëª¨ë“ˆì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    print("="*50)
    print("Core Analysis Engine - Module Self-Test")
    print("="*50)

    try:
        # 1. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        print("\n[Step 1] Preloading all models...")
        preload_models()
        print("\n[Step 1] Model loading test PASSED.")
    except Exception as e:
        print(f"\n[Step 1] Model loading test FAILED: {e}")
        sys.exit(1)

    try:
        # 2. ê°€ìƒ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± ë° ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n[Step 2] Creating dummy audio and running analysis...")
        # "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤"ì™€ ìœ ì‚¬í•œ í†¤ì˜ 3ì´ˆì§œë¦¬ ê°€ìƒ ì˜¤ë””ì˜¤ ìƒì„±
        sr = TARGET_SR
        duration = 3
        frequency1 = 120  # ë‚¨ì„± ëª©ì†Œë¦¬ í†¤
        frequency2 = 150
        t = np.linspace(0., duration, int(sr * duration), endpoint=False)
        amplitude = np.iinfo(np.int16).max * 0.1
        # ì‹œê°„ì— ë”°ë¼ ì£¼íŒŒìˆ˜ê°€ ë³€í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í†¤ ìƒì„±
        data = amplitude * (np.sin(2. * np.pi * (frequency1 + t * 10) * t) + 0.5 * np.sin(2. * np.pi * (frequency2 + t * 5) * t))
        dummy_audio = data.astype(np.float32) / np.iinfo(np.int16).max

        print(f"Generated {duration} seconds of dummy audio.")
        
        analysis_result = process_audio_simple(dummy_audio)
        
        print("\n--- Analysis Result (JSON) ---")
        print(json.dumps(analysis_result, indent=2, ensure_ascii=False))
        print("--------------------------------")

        if analysis_result and analysis_result['emotion'] != 'ì˜¤ë¥˜':
             print("\n[Step 2] Analysis pipeline test PASSED.")
        else:
            raise RuntimeError("Analysis result was empty or indicated an error.")

    except Exception as e:
        print(f"\n[Step 2] Analysis pipeline test FAILED: {e}")
        sys.exit(1)

    print("\nâœ… All tests completed successfully. The module is ready to be used.")
