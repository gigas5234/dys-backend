"""
Voice API - STTì™€ ì—°ë™ë˜ëŠ” API ì¸í„°í˜ì´ìŠ¤
ê¸°ì¡´ voice_input.pyì˜ process_audio_simple í•¨ìˆ˜ë¥¼ ëŒ€ì²´
"""

import logging
from typing import Dict, Any, Optional
import numpy as np
import torch
import torchaudio

from .voice_processor import VoiceProcessor
from .voice_scorer import ScoringWeights

logger = logging.getLogger(__name__)

# ì „ì—­ VoiceProcessor ì¸ìŠ¤í„´ìŠ¤
_voice_processor: Optional[VoiceProcessor] = None

def initialize_voice_processor(gpu_config: Optional[Dict] = None, weights: Optional[ScoringWeights] = None):
    """VoiceProcessor ì´ˆê¸°í™”"""
    global _voice_processor
    
    if _voice_processor is None:
        _voice_processor = VoiceProcessor(gpu_config, weights)
        logger.info("VoiceProcessor ì´ˆê¸°í™” ì™„ë£Œ")
    
    return _voice_processor

def get_voice_processor() -> VoiceProcessor:
    """VoiceProcessor ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _voice_processor
    
    if _voice_processor is None:
        logger.warning("VoiceProcessorê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìë™ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        initialize_voice_processor()
    
    return _voice_processor

def process_audio_simple(audio_array: np.ndarray, sr: int = 16000, elapsed_sec: float = 0.0) -> Dict[str, Any]:
    """
    ê¸°ì¡´ voice_input.pyì˜ process_audio_simple í•¨ìˆ˜ë¥¼ ëŒ€ì²´í•˜ëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜
    STTì™€ ì—°ë™ë˜ëŠ” í†µí•© ìŒì„± ë¶„ì„ (ë‹¤ì¤‘ STT ë°©ë²• ì§€ì›)
    
    Args:
        audio_array: ì˜¤ë””ì˜¤ ë°ì´í„° (numpy array)
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 16000)
        elapsed_sec: ê²½ê³¼ ì‹œê°„ (ì´ˆ) - ëŒ€í™” ë§¥ë½ ë¶„ì„ìš©
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ í˜•ì‹ê³¼ í˜¸í™˜)
    """
    try:
        processor = get_voice_processor()
        
        # ìŒì„± ë¶„ì„ ì‹¤í–‰
        result = processor.process_audio(audio_array, sr, elapsed_sec)
        
        # ê¸°ì¡´ í˜•ì‹ê³¼ í˜¸í™˜ë˜ë„ë¡ ê²°ê³¼ ë³€í™˜
        return {
            'transcript': result.get('transcript', ''),
            'emotion': result.get('emotion', 'ì¤‘ë¦½'),
            'emotion_score': result.get('emotion_score', 1.0),
            'total_score': result.get('total_score', 50.0),
            'voice_tone_score': result.get('voice_tone_score', 50.0),
            'voice_details': result.get('voice_details', {}),
            'word_choice_score': result.get('word_choice_score', 50.0),
            'word_details': result.get('word_details', {}),
            'weights': result.get('weights', {}),
            'positive_words': result.get('positive_words', []),
            'negative_words': []
        }
        
    except Exception as e:
        logger.error(f"process_audio_simple ì˜¤ë¥˜: {e}", exc_info=True)
        # ëŒ€ì•ˆ STT ë°©ë²• ì‹œë„
        return fallback_stt_analysis(audio_array, sr, elapsed_sec)

def fallback_stt_analysis(audio_array: np.ndarray, sr: int = 16000, elapsed_sec: float = 0.0) -> Dict[str, Any]:
    """
    ëŒ€ì•ˆ STT ë°©ë²•ì„ ì‚¬ìš©í•œ ìŒì„± ë¶„ì„ (fallback)
    """
    try:
        # 1. faster-whisper ì§ì ‘ ì‹œë„ (libctranslate2 ì˜¤ë¥˜ ë°©ì§€)
        try:
            from faster_whisper import WhisperModel
            import tempfile
            import os
            import torchaudio
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name
                torchaudio.save(temp_path, torch.tensor(audio_array).unsqueeze(0), sr)
            
            try:
                model = WhisperModel("base", device="cpu", compute_type="int8")
                segments, _ = model.transcribe(temp_path, language="ko")
                
                transcript = ""
                for segment in segments:
                    transcript += segment.text
                
                if transcript.strip():
                    logger.info(f"âœ… fallback STT ì„±ê³µ: {transcript}")
                    return create_fallback_result(transcript, "faster-whisper-direct")
                    
            finally:
                os.unlink(temp_path)
                
        except Exception as e:
            if "libctranslate2" in str(e).lower():
                logger.warning("âš ï¸ libctranslate2 ì˜¤ë¥˜ë¡œ faster-whisper fallback ê±´ë„ˆëœ€")
            else:
                logger.warning(f"faster-whisper fallback ì‹¤íŒ¨: {e}")
        
        # 2. OpenAI Whisper API ì‹œë„
        try:
            import tempfile
            import os
            import torchaudio
            
            if os.getenv('OPENAI_API_KEY'):
                # ì„ì‹œ íŒŒì¼ ìƒì„±
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    temp_path = temp_file.name
                    torchaudio.save(temp_path, torch.tensor(audio_array).unsqueeze(0), sr)
                
                try:
                    # OpenAI API í˜¸ì¶œ (ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ë°©ì‹)
                    from openai import OpenAI
                    
                        # í”„ë¡ì‹œ ì œê±° - OpenAI ì§ì ‘ ì—°ê²°ë¡œ ì•ˆì •ì„± í™•ë³´ (import ì œê±°)
                        print("ğŸ”— OpenAI í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì—°ê²° ì´ˆê¸°í™” (Voice API)")
                        client = OpenAI(
                            api_key=os.getenv('OPENAI_API_KEY'),
                            timeout=60.0
                        )
                        print("âœ… OpenAI ìŒì„± API ì§ì ‘ ì—°ê²° ì™„ë£Œ")
                        
                        with open(temp_path, 'rb') as audio_file:
                            response = client.audio.transcriptions.create(
                                model="whisper-1",
                                file=audio_file,
                                language="ko"
                            )
                        
                        transcript = response.text.strip()
                        if transcript:
                            logger.info(f"âœ… OpenAI Whisper fallback ì„±ê³µ: {transcript}")
                            return create_fallback_result(transcript, "openai-whisper")
                            
                    except ImportError:
                        # httpxê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
                        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                        
                        with open(temp_path, 'rb') as audio_file:
                            response = client.audio.transcriptions.create(
                                model="whisper-1",
                                file=audio_file,
                                language="ko"
                            )
                        
                        transcript = response.text.strip()
                        if transcript:
                            logger.info(f"âœ… OpenAI Whisper fallback ì„±ê³µ: {transcript}")
                            return create_fallback_result(transcript, "openai-whisper")
                            
                finally:
                    os.unlink(temp_path)
                    
        except Exception as e:
            logger.warning(f"OpenAI Whisper fallback ì‹¤íŒ¨: {e}")
        
        # 3. Google Speech-to-Text API ì‹œë„
        try:
            from google.cloud import speech
            import tempfile
            import os
            import torchaudio
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name
                torchaudio.save(temp_path, torch.tensor(audio_array).unsqueeze(0), sr)
            
            try:
                client = speech.SpeechClient()
                
                with open(temp_path, 'rb') as audio_file:
                    content = audio_file.read()
                
                audio = speech.RecognitionAudio(content=content)
                config = speech.RecognitionConfig(
                    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                    sample_rate_hertz=sr,
                    language_code="ko-KR",
                )
                
                response = client.recognize(config=config, audio=audio)
                
                transcript = ""
                for result in response.results:
                    transcript += result.alternatives[0].transcript
                
                if transcript.strip():
                    logger.info(f"âœ… Google Speech-to-Text fallback ì„±ê³µ: {transcript}")
                    return create_fallback_result(transcript, "google-speech")
                    
            finally:
                os.unlink(temp_path)
                
        except Exception as e:
            logger.warning(f"Google Speech-to-Text fallback ì‹¤íŒ¨: {e}")
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        logger.error("ëª¨ë“  STT ë°©ë²• ì‹¤íŒ¨")
        return create_fallback_result("ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "none")
        
    except Exception as e:
        logger.error(f"fallback_stt_analysis ì˜¤ë¥˜: {e}")
        return create_fallback_result("ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "error")

def create_fallback_result(transcript: str, method: str) -> Dict[str, Any]:
    """fallback ê²°ê³¼ ìƒì„±"""
    return {
        'transcript': transcript,
        'emotion': 'ì¤‘ë¦½',
        'emotion_score': 1.0,
        'total_score': 60.0,
        'voice_tone_score': 60.0,
        'voice_details': {'stt_method': method},
        'word_choice_score': 60.0,
        'word_details': {},
        'weights': {'voice': 0.4, 'word': 0.4, 'emotion': 0.2},
        'positive_words': [],
        'negative_words': []
    }

def analyze_voice_with_context(
    audio_array: np.ndarray, 
    sr: int = 16000, 
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    ë§¥ë½ ì •ë³´ë¥¼ í¬í•¨í•œ ìŒì„± ë¶„ì„
    
    Args:
        audio_array: ì˜¤ë””ì˜¤ ë°ì´í„°
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        context: ë§¥ë½ ì •ë³´ (ëŒ€í™” ì‹œì‘ ì‹œê°„, ì´ì „ ë¶„ì„ ê²°ê³¼ ë“±)
        
    Returns:
        ë§¥ë½ì„ ê³ ë ¤í•œ ë¶„ì„ ê²°ê³¼
    """
    try:
        processor = get_voice_processor()
        
        # ë§¥ë½ ì •ë³´ ì¶”ì¶œ
        elapsed_sec = 0.0
        if context:
            elapsed_sec = context.get('elapsed_sec', 0.0)
        
        # ìŒì„± ë¶„ì„ ì‹¤í–‰
        result = processor.process_audio(audio_array, sr, elapsed_sec)
        
        # ë§¥ë½ ì •ë³´ ì¶”ê°€
        if context:
            result['context'] = context
        
        return result
        
    except Exception as e:
        logger.error(f"analyze_voice_with_context ì˜¤ë¥˜: {e}", exc_info=True)
        return process_audio_simple(audio_array, sr)

def get_voice_analysis_summary(audio_array: np.ndarray, sr: int = 16000) -> Dict[str, Any]:
    """ìŒì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½ (UI í‘œì‹œìš©)"""
    try:
        processor = get_voice_processor()
        result = processor.process_audio(audio_array, sr)
        return processor.get_analysis_summary(result)
        
    except Exception as e:
        logger.error(f"get_voice_analysis_summary ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            'transcript': 'ë¶„ì„ ì‹¤íŒ¨',
            'emotion': 'ì¤‘ë¦½',
            'total_score': 50.0,
            'overall_mood': 'ë³´í†µ',
            'voice_tone_score': 50.0,
            'word_choice_score': 50.0,
            'processing_time': 0.0,
            'audio_duration': 0.0
        }

def get_voice_detailed_analysis(audio_array: np.ndarray, sr: int = 16000) -> Dict[str, Any]:
    """ìƒì„¸ ìŒì„± ë¶„ì„ ê²°ê³¼ (íŒì—… í‘œì‹œìš©)"""
    try:
        processor = get_voice_processor()
        result = processor.process_audio(audio_array, sr)
        return processor.get_detailed_analysis(result)
        
    except Exception as e:
        logger.error(f"get_voice_detailed_analysis ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            'summary': get_voice_analysis_summary(audio_array, sr),
            'voice_details': {},
            'word_details': {},
            'evidence': {'ì˜¤ë¥˜': 'ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨'},
            'recommendations': ['ë¶„ì„ì„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”'],
            'positive_words': [],
            'negative_words': [],
            'weights': {'voice': 0.4, 'word': 0.4, 'emotion': 0.2},
            'audio_quality': {}
        }

def is_voice_processor_ready() -> bool:
    """VoiceProcessor ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
    try:
        processor = get_voice_processor()
        return processor.is_ready()
    except Exception as e:
        logger.error(f"VoiceProcessor ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
        return False

def get_voice_model_status() -> Dict[str, Any]:
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        processor = get_voice_processor()
        return processor.get_model_status()
    except Exception as e:
        logger.error(f"ëª¨ë¸ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
        return {
            'models_loaded': False,
            'analyzer_ready': False,
            'stt_available': False,
            'stt_method': 'none',
            'asr_model': False,
            'audio_classifier': False
        }

def preload_voice_models():
    """ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
    try:
        processor = get_voice_processor()
        processor.load_models()
        logger.info("ìŒì„± ë¶„ì„ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False

# ê¸°ì¡´ voice_input.pyì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
def preload_models():
    """ê¸°ì¡´ preload_models í•¨ìˆ˜ì™€ í˜¸í™˜"""
    return preload_voice_models()

# ëª¨ë“ˆ ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ VoiceProcessor ìƒì„±
initialize_voice_processor()
