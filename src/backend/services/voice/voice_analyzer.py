"""
Voice Analyzer - ìŒì„± ë¶„ì„ í•µì‹¬ ëª¨ë“ˆ
3ê°œ íŒŒì¼(voice_input.py, simple_mic_test.py, infer_emotion.py)ì˜ ë¡œì§ì„ ì¢…í•©
"""

import os
import sys
import logging
import json
import time
import threading
import queue
from typing import Tuple, Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime

import numpy as np
import torch
import torchaudio
from faster_whisper import WhisperModel

# TransformersëŠ” ì„ íƒì ìœ¼ë¡œ import
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ì‚¬ìš©")

# OpenAI API ëŒ€ì•ˆ ì¶”ê°€
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# faster-whisper ì‹œë„ (libctranslate2 ì˜¤ë¥˜ ë°©ì§€)
FASTER_WHISPER_AVAILABLE = False
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
    print("âœ… faster-whisper ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ faster-whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
    FASTER_WHISPER_AVAILABLE = False
except Exception as e:
    if "libctranslate2" in str(e).lower():
        print("âš ï¸ libctranslate2 ì˜¤ë¥˜ë¡œ ì¸í•´ faster-whisper ë¹„í™œì„±í™”")
        FASTER_WHISPER_AVAILABLE = False
    else:
        print(f"âš ï¸ faster-whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
        FASTER_WHISPER_AVAILABLE = False

# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/tmp/voice_analysis.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
TARGET_SR = 16000  # ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë ˆì´íŠ¸ (16kHz)

# ê°ì • ë ˆì´ë¸” ë§¤í•‘ (ì˜ì–´ -> í•œêµ­ì–´)
AUDIO_LABEL_MAP = {
    "angry": "ë¶„ë…¸", "calm": "ì¤‘ë¦½", "disgust": "í˜ì˜¤", "fearful": "ë‘ë ¤ì›€",
    "happy": "ê¸°ì¨", "neutral": "ì¤‘ë¦½", "sad": "ìŠ¬í””", "surprise": "ë†€ëŒ"
}

# í•œêµ­ì–´ ê°ì • ë ˆì´ë¸”
KOREAN_EMOTION_LABELS = [
    "ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ì¤‘ë¦½", "ë‘ë ¤ì›€", "í˜ì˜¤", "ë†€ëŒ",
    "ì§œì¦", "ê·€ì°®ìŒ", "ì„œìš´í•¨", "ì–µìš¸í•¨", "ë¶ˆì•ˆ", "ë¬´ì‹¬í•¨"
]

# ê¸ì •ì  ë‹¨ì–´ ì‚¬ì „ (ë°ì´íŠ¸/ê³µê°í˜•ì„±ìš©)
POSITIVE_DATING_WORDS = {
    "ì¢‹ì•„": 0.8, "ì¢‹ì€": 0.8, "ì¢‹ë‹¤": 0.8, "ì¢‹ë„¤": 0.8, "ì¢‹ì•„ìš”": 0.9,
    "ê°ì‚¬": 0.9, "ê³ ë§ˆì›Œ": 0.9, "ê³ ë§ˆì›Œìš”": 0.9, "ê°ì‚¬í•´": 0.9, "ê°ì‚¬í•©ë‹ˆë‹¤": 1.0,
    "í–‰ë³µ": 0.9, "ì¦ê±°ì›Œ": 0.8, "ì¦ê±°ìš´": 0.8, "ì¬ë¯¸ìˆ": 0.8, "ì¬ë°Œ": 0.8,
    "ì‚¬ë‘": 1.0, "ì‚¬ë‘í•´": 1.0, "ì‚¬ë‘í•´ìš”": 1.0, "ì‚¬ë‘ìŠ¤ëŸ¬ì›Œ": 1.0,
    "ì˜ˆì˜": 0.9, "ì•„ë¦„ë‹¤ìš´": 0.9, "ë©‹ìˆ": 0.8, "ë©‹ì§„": 0.8, "í›Œë¥­": 0.8,
    "ì™„ë²½": 0.9, "ì™„ë²½í•´": 0.9, "ì™„ë²½í•œ": 0.9, "ìµœê³ ": 0.9, "ìµœê³ ì•¼": 0.9,
    "íŠ¹ë³„": 0.8, "íŠ¹ë³„í•œ": 0.8, "íŠ¹ë³„í•´": 0.8, "ì†Œì¤‘": 0.9, "ì†Œì¤‘í•œ": 0.9,
    "ê¸°ì˜": 0.8, "ê¸°ë»": 0.8, "ê¸°ë»ìš”": 0.8, "ì‹ ë‚˜": 0.7, "ì‹ ë‚˜ìš”": 0.7,
    "ê´œì°®": 0.6, "ê´œì°®ì•„": 0.6, "ê´œì°®ì•„ìš”": 0.6, "ê´œì°®ì€": 0.6,
    "ë§ì•„": 0.7, "ë§ì•„ìš”": 0.7, "ê·¸ë˜": 0.6, "ê·¸ë˜ìš”": 0.6, "ë„¤": 0.5,
    "ì‘": 0.4, "ì–´": 0.3, "ìŒ": 0.2, "ê·¸ë ‡": 0.6, "ê·¸ë ‡ë„¤": 0.6,
    "ì •ë§": 0.7, "ì§„ì§œ": 0.7, "ë„ˆë¬´": 0.6, "ë§¤ìš°": 0.7, "ì •ë§ë¡œ": 0.8,
    "~ë„¤ìš”": 0.3, "~ì–´ìš”": 0.3, "~ì•„ìš”": 0.3, "~ì…ë‹ˆë‹¤": 0.4, "~ìŠµë‹ˆë‹¤": 0.4,
    "í™˜ì˜": 0.8, "í™˜ì˜í•´": 0.8, "í™˜ì˜í•©ë‹ˆë‹¤": 0.9,
    "ë°˜ê°€ì›Œ": 0.8, "ë°˜ê°‘ìŠµë‹ˆë‹¤": 0.9,
    "ì¹­ì°¬": 0.8, "ëŒ€ë‹¨í•´": 0.9, "ë©‹ì ¸": 0.8, "ë†€ë¼ì›Œ": 0.9,
    "ìë‘ìŠ¤ëŸ¬ì›Œ": 0.9, "ë¯¿ìŒì§": 0.8,
    "ë“ ë“ í•´": 0.8, "í–‰ìš´": 0.8, "ì¶•í•˜": 0.9, "ì¶•í•˜í•´": 0.9, "ì¶•í•˜í•©ë‹ˆë‹¤": 1.0,
    "í¸ì•ˆ": 0.7, "ì•ˆì‹¬": 0.7, "ë”°ëœ»": 0.8, "ë“ ë“ ": 0.8,
    "ê¸ì •ì ": 0.8, "í™œê¸°ì°¬": 0.8, "ë°ì€": 0.8, "ìš©ê¸°": 0.8, "í¬ë§": 0.9
}

# ë¶€ì •ì  ë‹¨ì–´ ì‚¬ì „
NEGATIVE_DATING_WORDS = {
    "ì‹«ì–´": -0.8, "ì‹«ì€": -0.8, "ì‹«ë‹¤": -0.8, "ì‹«ë„¤": -0.8, "ì‹«ì–´ìš”": -0.9,
    "í™”ë‚˜": -0.9, "í™”ë‚˜ìš”": -0.9, "í™”ê°€": -0.9, "ì§œì¦": -0.8, "ì§œì¦ë‚˜": -0.8,
    "ê·€ì°®": -0.7, "ê·€ì°®ì•„": -0.7, "ê·€ì°®ì€": -0.7, "ì§€ê²¨ì›Œ": -0.7, "ì§€ê²¨ìš´": -0.7,
    "ë‹µë‹µ": -0.6, "ë‹µë‹µí•´": -0.6, "ë‹µë‹µí•œ": -0.6, "í˜ë“¤": -0.6, "í˜ë“¤ì–´": -0.6,
    "ì–´ë ¤ì›Œ": -0.5, "ì–´ë ¤ìš´": -0.5, "ë³µì¡": -0.4, "ë³µì¡í•´": -0.4, "ë³µì¡í•œ": -0.4,
    "ì•„í”„": -0.6, "ì•„íŒŒ": -0.6, "ì•„í”ˆ": -0.6, "í”¼ê³¤": -0.5, "í”¼ê³¤í•´": -0.5,
    "ì§€ì¹˜": -0.5, "ì§€ì³": -0.5, "ì§€ì¹œ": -0.5, "ìŠ¤íŠ¸ë ˆìŠ¤": -0.6, "ìŠ¤íŠ¸ë ˆìŠ¤ë°›": -0.6,
    "ë¶ˆì•ˆ": -0.7, "ë¶ˆì•ˆí•´": -0.7, "ë¶ˆì•ˆí•œ": -0.7, "ê±±ì •": -0.6, "ê±±ì •ë¼": -0.6,
    "ìŠ¬í¼": -0.7, "ìŠ¬í”ˆ": -0.7, "ìš°ìš¸": -0.7, "ìš°ìš¸í•´": -0.7, "ìš°ìš¸í•œ": -0.7,
    "ë¬´ì„œì›Œ": -0.6, "ë¬´ì„œìš´": -0.6, "ë‘ë ¤ì›Œ": -0.6, "ë‘ë ¤ìš´": -0.6,
    "~ì§€ë§ˆ": -0.5, "~í•˜ì§€ë§ˆ": -0.5, "~ë§ˆ": -0.4, "~ì§€ ë§ê³ ": -0.3,
    "ì§œì¦ìŠ¤ëŸ¬ì›Œ": -0.8, "ë¶ˆí¸": -0.6, "ë¶ˆí¸í•´": -0.6,
    "ì‹«ì¦": -0.7, "ì§€ë£¨í•´": -0.6, "ì§€ë£¨í•œ": -0.6,
    "í›„íšŒ": -0.7, "í›„íšŒí•´": -0.7,
    "ì‹¤ë§": -0.7, "ì‹¤ë§í–ˆì–´": -0.7, "ì‹¤ë§ì´ì•¼": -0.7,
    "ê´´ë¡œì›Œ": -0.7, "ê´´ë¡œìš´": -0.7,
    "ì§œì¦ìŠ¤ëŸ½ë‹¤": -0.8, "í˜ê²¨ì›Œ": -0.7, "ë¶ˆí–‰": -0.8,
    "ì™¸ë¡œì›Œ": -0.6, "ì™¸ë¡œìš´": -0.6,
    "ì°¨ê°‘": -0.5, "ëƒ‰ì •": -0.6, "ì‹«ì¦ë‚˜": -0.6
}

# ê³µì†í•œ í‘œí˜„ ì‚¬ì „
POLITE_PHRASES = [
    "ê°ì‚¬í•©ë‹ˆë‹¤", "ê³ ë§™ìŠµë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤", "ë¯¸ì•ˆí•©ë‹ˆë‹¤", "ì‹¤ë¡€í•©ë‹ˆë‹¤",
    "ë¶€íƒë“œë¦½ë‹ˆë‹¤", "ë¶€íƒí•©ë‹ˆë‹¤", "ë„ì™€ì£¼ì„¸ìš”", "ì•Œë ¤ì£¼ì„¸ìš”", "ê°€ë¥´ì³ì£¼ì„¸ìš”",
    "~í•´ì£¼ì„¸ìš”", "~í•´ì£¼ì‹œë©´", "~í•´ì£¼ì‹œëŠ”", "~í•´ì£¼ì…”ì„œ", "~í•´ì£¼ì‹ ",
    "~ì‹œê² ì–´ìš”", "~ì‹œê² ìŠµë‹ˆê¹Œ", "~ì‹œëŠ”êµ°ìš”", "~ì‹œëŠ”êµ¬ë‚˜", "~ì‹œëŠ”ì§€",
    "ì •ì¤‘íˆ", "ì‹¤ë¡€ì§€ë§Œ", "ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤", "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤",
    "ê³ ìƒí•˜ì…¨ìŠµë‹ˆë‹¤", "ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤", "ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤",
    "ë¶€íƒë“œë ¤ìš”", "ê°ì‚¬ë“œë ¤ìš”", "ë„ì›€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
    "ë„ì™€ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬ ì¸ì‚¬ë“œë¦½ë‹ˆë‹¤"
]

# ê³µê° í‘œí˜„ ì‚¬ì „
EMPATHY_INDICATORS = [
    "ê·¸ë ‡êµ¬ë‚˜", "ê·¸ëŸ°ê°€ë´", "ê·¸ëŸ° ê²ƒ ê°™ì•„", "ì´í•´í•´", "ì´í•´ê°€ ë¼",
    "ê³µê°í•´", "ê³µê°ì´ ë¼", "ê°™ì€ ìƒê°ì´ì•¼", "ë‚˜ë„ ê·¸ë˜", "ë‚˜ë„ ê·¸ë¬ì–´",
    "í˜ë“¤ì—ˆê² ë‹¤", "ì–´ë ¤ì› ê² ë‹¤", "ê´œì°®ì„ ê±°ì•¼", "ê´œì°®ì•„ì§ˆ ê±°ì•¼", "ì˜ ë  ê±°ì•¼",
    "ë„ì›€ì´ ë ê¹Œ", "ë¬´ì—‡ì„ ë„ì™€ì¤„ê¹Œ", "ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ", "ì–´ë–»ê²Œ ìƒê°í•´",
    "ì–´ë–»ê²Œ ëŠê»´", "ì–´ë–¤ ê¸°ë¶„ì´ì•¼", "ì–´ë–¤ ìƒê°ì´ì•¼",
    "ê³ ìƒ ë§ì•˜ì–´", "ê³ ìƒí–ˆê² ë‹¤", "ì†ìƒí–ˆê² ë‹¤", "ì†ìƒí–ˆì„ ê²ƒ ê°™ì•„",
    "ë§ˆìŒ ì•„íŒ ê² ë‹¤", "ê±±ì • ë§ì•˜ê² ë‹¤", "ê±±ì •í–ˆì§€", "ì–¼ë§ˆë‚˜ í˜ë“¤ì—ˆì„ê¹Œ",
    "ë§ì´ í˜ë“¤ì—ˆê² ë‹¤", "ê·¸ëŸ´ ìˆ˜ë„ ìˆê² ë‹¤", "ì¶©ë¶„íˆ ì´í•´í•´", "ë‚˜ë„ ëŠê»´",
    "ë‚˜ë„ ê²ªì–´ë´¤ì–´", "ë„¤ ì…ì¥ ì´í•´í•´", "ë„¤ ê¸°ë¶„ ì´í•´í•´",
    "ê´œì°®ì•„ ê´œì°®ì•„", "ë„ˆ ì˜í•˜ê³  ìˆì–´", "ê³ ë§ˆì›Œ ê·¸ëŸ° ì–˜ê¸°í•´ì¤˜ì„œ"
]

# ì—´ì • í‘œí˜„ ì‚¬ì „
ENTHUSIASM_INDICATORS = [
    "ì™€!", "ìš°ì™€!", "ëŒ€ë°•!", "ì§„ì§œ?", "ì •ë§?", "ì™€ìš°!", "ë†€ë¼ì›Œ!",
    "ì‹ ê¸°í•´!", "ë©‹ì ¸!", "ì™„ë²½í•´!", "ìµœê³ ì•¼!", "íŠ¹ë³„í•´!", "ì†Œì¤‘í•´!",
    "ê¸°ë»!", "ì‹ ë‚˜!", "ì¦ê±°ì›Œ!", "ì¬ë°Œì–´!", "ì¬ë¯¸ìˆì–´!", "í–‰ë³µí•´!",
    "ì‚¬ë‘í•´!", "ì˜ˆë»!", "ì•„ë¦„ë‹¤ì›Œ!", "í›Œë¥­í•´!", "ì™„ë²½í•´!", "ìµœê³ ì•¼!",
    "êµ‰ì¥í•´!", "ì—„ì²­ë‚˜!", "í™˜ìƒì ì´ì•¼!", "ë¯¿ì„ ìˆ˜ ì—†ì–´!", "ì©”ì–´!",
    "ëë‚´ì¤˜!", "í¥ë¯¸ì§„ì§„í•´!", "ë†€ëë‹¤!", "ê°ë™ì´ì•¼!", "ìµœê°•ì´ì•¼!",
    "ì™„ì „ ë©‹ì ¸!", "ë ˆì „ë“œì•¼!", "ì••ë„ì ì´ì•¼!", "ì£½ì—¬ì¤€ë‹¤!", "ìµœìƒê¸‰ì´ì•¼!",
    "ì—´ê´‘ì ì´ì•¼!", "ì§œë¦¿í•´!", "ì§±ì´ì•¼!", "ì™„ì „ ì¢‹ì•„!", "ì¢‹ì•„ ì£½ê² ë‹¤!"
]

@dataclass
class VoiceToneAnalysis:
    """ìŒì„± í†¤ ë¶„ì„ ê²°ê³¼"""
    pitch_variation: float      # 0-1, ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ í’ë¶€
    speaking_speed: float       # ì´ˆë‹¹ ë‹¨ì–´ ìˆ˜
    volume_consistency: float   # 0-1, ë†’ì„ìˆ˜ë¡ ì•ˆì •ì 
    warmth_score: float         # 0-1, ë†’ì„ìˆ˜ë¡ ë”°ëœ»í•¨
    enthusiasm_level: float     # 0-1, ë†’ì„ìˆ˜ë¡ ì—´ì •ì 
    politeness_level: float     # 0-1, ë†’ì„ìˆ˜ë¡ ê³µì†í•¨
    confidence_level: float     # 0-1, ë†’ì„ìˆ˜ë¡ ìì‹ ê°
    volume_strength: float      # 0-1, ë†’ì„ìˆ˜ë¡ ê°•í•œ ë³¼ë¥¨

@dataclass
class WordChoiceAnalysis:
    """ë‹¨ì–´ ì„ íƒ ë¶„ì„ ê²°ê³¼"""
    positive_words: List[str]
    negative_words: List[str]
    polite_phrases: List[str]
    empathy_indicators: List[str]
    enthusiasm_indicators: List[str]
    politeness_score: float     # 0-1
    empathy_score: float        # 0-1
    enthusiasm_score: float     # 0-1
    valence_score: float        # 0-1, ë§¥ë½/ì‹œê°„/í†¤ ë°˜ì˜í•œ ì •ì„œ ì ìˆ˜

@dataclass
class VoiceAnalysisResult:
    """ìŒì„± ë¶„ì„ ì¢…í•© ê²°ê³¼"""
    transcript: str
    emotion: str
    emotion_score: float
    voice_tone: VoiceToneAnalysis
    word_choice: WordChoiceAnalysis
    total_score: float
    voice_tone_score: float
    word_choice_score: float
    overall_mood: str
    evidence: Dict[str, str]
    recommendations: List[str]
    processing_time: float
    audio_quality: Dict[str, Any]

class VoiceAnalyzer:
    """ìŒì„± ë¶„ì„ í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self, gpu_config: Optional[Dict] = None):
        self.gpu_config = gpu_config or self._detect_gpu_config()
        self._asr_model: Optional[WhisperModel] = None
        self._nli_tokenizer = None
        self._nli_model = None
        self._audio_clf = None
        self._models_loaded = False
        
        logger.info(f"VoiceAnalyzer ì´ˆê¸°í™” - GPU ì„¤ì •: {self.gpu_config['name']}")
    
    def _detect_gpu_config(self) -> Dict:
        """GPU ì„¤ì • ìë™ ê°ì§€"""
        if not torch.cuda.is_available():
            return {
                "name": "CPU",
                "memory_gb": 0,
                "asr_model_size": "small",
                "asr_compute_type": "int8",
                "nli_model_id": "joeddav/xlm-roberta-large-xnli",
                "max_seconds": 20,
                "batch_size": 1
            }
        
        gpu_name = torch.cuda.get_device_name(0).lower()
        memory_gb = torch.cuda.get_device_properties(0).total_memory // (1024**3)
        
        if "4080" in gpu_name or memory_gb >= 16:
            return {
                "name": "RTX 4080 20GB",
                "memory_gb": 20,
                "asr_model_size": "large-v3",
                "asr_compute_type": "float16",
                "nli_model_id": "joeddav/xlm-roberta-large-xnli",
                "max_seconds": 30,
                "batch_size": 4
            }
        else:
            return {
                "name": "1660 Super 6GB",
                "memory_gb": 6,
                "asr_model_size": "small",
                "asr_compute_type": "int8",
                "nli_model_id": "joeddav/xlm-roberta-large-xnli",
                "max_seconds": 20,
                "batch_size": 1
            }
    
    def load_models(self):
        """ëª¨ë“  AI ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ (ì²« ë²ˆì§¸ ì„±ê³µ ëª¨ë¸ ì±„íƒ)"""
        if self._models_loaded:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        logger.info("ëª¨ë¸ ë¡œë”© ì‹œì‘... (ì²« ë²ˆì§¸ ì„±ê³µ ëª¨ë¸ ì±„íƒ)")
        
        # GKE í™˜ê²½ì—ì„œëŠ” CPU ì‚¬ìš©ì„ ìš°ì„ 
        device = "cpu"  # GKE í™˜ê²½ì—ì„œëŠ” CPU ì‚¬ìš©
        logger.info(f"ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        logger.info("ğŸŒ GKE í™˜ê²½ì—ì„œ CPU ê¸°ë°˜ ìŒì„± ë¶„ì„ ì¤€ë¹„")
        
        # 1. ASR ëª¨ë¸ (ì²« ë²ˆì§¸ ì„±ê³µ ëª¨ë¸ ì±„íƒ)
        self._asr_model = None
        self._stt_method = "none"
        
        # ë°©ë²• 1: faster-whisper ì‹œë„ (libctranslate2 ì˜¤ë¥˜ ë°©ì§€)
        global FASTER_WHISPER_AVAILABLE
        if FASTER_WHISPER_AVAILABLE:
            try:
                self._asr_model = WhisperModel("tiny", device="cpu", compute_type="int8", num_workers=2)
                self._stt_method = "faster-whisper-tiny"
                logger.info("âœ… ASR ëª¨ë¸ ë¡œë“œ ì„±ê³µ (faster-whisper tiny - ì„±ëŠ¥ ìµœì í™”)")
                logger.info("ğŸ¤ faster-whisper tiny ëª¨ë¸ ì±„íƒ (2ë°° ë¹ ë¦„, 2 workers)")
            except Exception as e:
                logger.warning(f"âš ï¸ faster-whisper base ë¡œë“œ ì‹¤íŒ¨: {e}")
                if "libctranslate2" in str(e).lower():
                    logger.warning("âš ï¸ libctranslate2 ì˜¤ë¥˜ë¡œ faster-whisper ë¹„í™œì„±í™”")
                    FASTER_WHISPER_AVAILABLE = False
                else:
                    # tiny ëª¨ë¸ë¡œ ì¬ì‹œë„
                    try:
                        self._asr_model = WhisperModel("tiny", device="cpu", compute_type="int8")
                        self._stt_method = "faster-whisper-tiny"
                        logger.info("âœ… ASR ëª¨ë¸ ë¡œë“œ ì„±ê³µ (faster-whisper tiny)")
                        logger.info("ğŸ¤ faster-whisper tiny ëª¨ë¸ ì±„íƒ")
                    except Exception as e2:
                        logger.warning(f"âš ï¸ faster-whisper tinyë„ ì‹¤íŒ¨: {e2}")
                        if "libctranslate2" in str(e2).lower():
                            logger.warning("âš ï¸ libctranslate2 ì˜¤ë¥˜ë¡œ faster-whisper tinyë„ ë¹„í™œì„±í™”")
                            FASTER_WHISPER_AVAILABLE = False
        
        # ë°©ë²• 2: OpenAI Whisper API ì‹œë„ (faster-whisper ì‹¤íŒ¨ ì‹œ)
        if self._asr_model is None and OPENAI_AVAILABLE:
            try:
                # OpenAI API í‚¤ í™•ì¸
                if os.getenv('OPENAI_API_KEY'):
                    self._stt_method = "openai-whisper"
                    logger.info("âœ… OpenAI Whisper API ì¤€ë¹„ ì™„ë£Œ")
                    logger.info("ğŸ¤ OpenAI Whisper API ì±„íƒ")
                else:
                    logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            except Exception as e:
                logger.warning(f"âš ï¸ OpenAI Whisper API ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 3: Google Speech-to-Text API ì‹œë„ (ì´ì „ ë°©ë²•ë“¤ ì‹¤íŒ¨ ì‹œ)
        if self._asr_model is None and self._stt_method == "none":
            try:
                from google.cloud import speech
                self._stt_method = "google-speech"
                logger.info("âœ… Google Speech-to-Text API ì¤€ë¹„ ì™„ë£Œ")
                logger.info("ğŸ¤ Google Speech-to-Text API ì±„íƒ")
            except ImportError:
                logger.info("â„¹ï¸ Google Speech-to-Text API ë¯¸ì„¤ì¹˜")
            except Exception as e:
                logger.warning(f"âš ï¸ Google Speech-to-Text API ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # 2. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ëª¨ë¸ (í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”)
        logger.info("í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ì‚¬ìš© (GKE í™˜ê²½ ìµœì í™”)")
        self._nli_tokenizer = None
        self._nli_model = None
        logger.info("ğŸ’¡ GKE í™˜ê²½ì—ì„œ ê°€ë²¼ìš´ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ì‚¬ìš©")
        
        # 3. ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ (GKE í™˜ê²½ì—ì„œëŠ” ë¹„í™œì„±í™”)
        logger.info("ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ ë¹„í™œì„±í™” (í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ìš© - GKE ìµœì í™”)")
        self._audio_clf = None
        logger.info("ğŸµ GKE í™˜ê²½ì—ì„œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´")
        
        # STT ëª¨ë¸ ìƒíƒœ í™•ì¸
        if self._stt_method != "none":
            logger.info(f"âœ… STT ëª¨ë¸ ì±„íƒ ì™„ë£Œ: {self._stt_method}")
        else:
            logger.warning("âš ï¸ ëª¨ë“  STT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ìŒì„± ì¸ì‹ ì œí•œë¨")
        
        self._models_loaded = True
        logger.info("ğŸ¯ ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì²« ë²ˆì§¸ ì„±ê³µ ëª¨ë¸ ì±„íƒ)")
        logger.info(f"ğŸ“Š ì±„íƒëœ ëª¨ë¸: STT={self._stt_method}, ê°ì •ë¶„ì„=í‚¤ì›Œë“œê¸°ë°˜")
        logger.info("ğŸš€ GKE í™˜ê²½ì—ì„œ STT ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ")
        logger.info("ğŸ‰ ìŒì„± ì…ë ¥ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    def analyze_voice_tone(self, audio_array: np.ndarray, sr: int) -> VoiceToneAnalysis:
        """ìŒì„± í†¤ì˜ ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ë¶„ì„"""
        try:
            # FFTë¥¼ ì´ìš©í•œ ìŒë†’ì´ ë³€í™”ëŸ‰ ì¶”ì •
            fft = np.fft.fft(audio_array)
            freqs = np.fft.fftfreq(len(audio_array), 1/sr)
            positive_freqs = freqs[freqs > 0]
            positive_fft = np.abs(fft[freqs > 0])
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì°¾ê¸°
            if len(positive_freqs) > 0:
                dominant_freq_idx = np.argmax(positive_fft)
                dominant_freq = positive_freqs[dominant_freq_idx]
                
                # ì£¼íŒŒìˆ˜ ë³€í™”ëŸ‰ ê³„ì‚°
                freq_variance = np.var(positive_freqs[positive_fft > np.max(positive_fft) * 0.1])
                pitch_variation = min(1.0, freq_variance / 10000)
            else:
                pitch_variation = 0.0
            
            # ë³¼ë¥¨ ì¼ê´€ì„± ê³„ì‚°
            chunk_size = int(0.1 * sr)  # 100ms chunks
            volumes = []
            for i in range(0, len(audio_array), chunk_size):
                chunk = audio_array[i:i+chunk_size]
                if len(chunk) > 0:
                    rms = np.sqrt(np.mean(np.square(chunk)))
                    volumes.append(rms)
            
            if len(volumes) > 1:
                volume_std = np.std(volumes)
                volume_consistency = max(0.0, 1.0 - volume_std / 0.1)
            else:
                volume_consistency = 0.5
            
            # ë§í•˜ê¸° ì†ë„ ì¶”ì • (ê°„ì†Œí™”ëœ ë²„ì „)
            speaking_speed = 3.0  # ê¸°ë³¸ê°’
            
            # ë”°ëœ»í•¨ ì ìˆ˜ (ì €ì£¼íŒŒ ì—ë„ˆì§€ ê¸°ë°˜)
            low_freq_energy = np.sum(np.abs(fft[freqs < 500])) / len(fft)
            high_freq_energy = np.sum(np.abs(fft[freqs > 2000])) / len(fft)
            warmth_score = min(1.0, low_freq_energy / (high_freq_energy + 1e-6))
            
            # ì—´ì • ìˆ˜ì¤€ (ì—ë„ˆì§€ ë³€í™”ëŸ‰ ê¸°ë°˜)
            energy_variation = np.std(np.abs(audio_array))
            enthusiasm_level = min(1.0, energy_variation / 0.1)
            
            # ê³µì†í•¨ ìˆ˜ì¤€ (ë³¼ë¥¨ ì¼ê´€ì„±ê³¼ í”¼ì¹˜ ë³€í™”ëŸ‰ ê¸°ë°˜)
            politeness_level = (volume_consistency + (1.0 - pitch_variation)) / 2
            
            # ìì‹ ê° ìˆ˜ì¤€ (ì—¬ëŸ¬ ìš”ì†Œ ì¢…í•©)
            overall_rms = np.sqrt(np.mean(np.square(audio_array)))
            volume_strength = min(1.0, overall_rms / 0.1)
            
            confidence_factors = [
                volume_strength * 0.4,
                volume_consistency * 0.3,
                min(1.0, energy_variation * 2) * 0.3
            ]
            confidence_level = sum(confidence_factors)
            
            return VoiceToneAnalysis(
                pitch_variation=pitch_variation,
                speaking_speed=speaking_speed,
                volume_consistency=volume_consistency,
                warmth_score=warmth_score,
                enthusiasm_level=enthusiasm_level,
                politeness_level=politeness_level,
                confidence_level=confidence_level,
                volume_strength=volume_strength
            )
            
        except Exception as e:
            logger.error(f"ìŒì„± í†¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return VoiceToneAnalysis(0.5, 3.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)
    
    def analyze_word_choice(self, text: str, elapsed_sec: float = 0.0, voice: Optional[VoiceToneAnalysis] = None) -> WordChoiceAnalysis:
        """ë§¥ë½/ì‹œê°„/ìŒì„±í†¤ì„ ë°˜ì˜í•œ ë‹¨ì–´ ì„ íƒ ë¶„ì„"""
        if not text:
            return WordChoiceAnalysis(
                positive_words=[], negative_words=[], polite_phrases=[],
                empathy_indicators=[], enthusiasm_indicators=[],
                politeness_score=0.5, empathy_score=0.5, enthusiasm_score=0.5,
                valence_score=0.5
            )
        
        text_norm = text.strip()
        text_lower = text_norm.lower()
        
        # ê¸ì •/ë¶€ì • ë‹¨ì–´ ìŠ¤ìº”
        positive_words = [w for w, s in POSITIVE_DATING_WORDS.items() if w in text_lower]
        negative_words = [w for w, s in NEGATIVE_DATING_WORDS.items() if w in text_lower]
        
        pos_raw_score = sum(POSITIVE_DATING_WORDS.get(w, 0) for w in positive_words)
        neg_raw_score = sum(abs(NEGATIVE_DATING_WORDS.get(w, 0)) for w in negative_words)
        
        # ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš©
        early = elapsed_sec < 30.0
        late = elapsed_sec >= 120.0
        
        # ì´ˆë°˜ ê³¼ì‰ ê¸ì •ì€ ê°ì 
        if early and any(tok in text_norm for tok in ["ì‚¬ë‘", "ê²°í˜¼", "ì˜ì›", "í‰ìƒ", "ìš´ëª…"]):
            pos_raw_score *= 0.6
            neg_raw_score += 0.3
        
        # ì´ˆë°˜ ë¶€ì •ì€ ê°€ì¤‘
        if early and neg_raw_score > 0:
            neg_raw_score *= 1.2
        
        # í›„ë°˜ë¶€ ë¶€ì •ì€ ì™„í™”
        if late and neg_raw_score > 0:
            neg_raw_score *= 0.8
        
        # ìŒì„± í†¤ ê²°í•©
        if voice is not None:
            tone_warm_bright = 0.6 * voice.warmth_score + 0.4 * voice.enthusiasm_level
            neg_raw_score *= (1.0 - 0.6 * min(1.0, max(0.0, tone_warm_bright)))
            if tone_warm_bright < 0.4:
                pos_raw_score *= 0.8
        
        # ê³µì†/ê³µê°/ì—´ì • ìŠ¤ì½”ì–´
        polite_phrases = [p for p in POLITE_PHRASES if p in text_norm]
        empathy_indicators = [e for e in EMPATHY_INDICATORS if e in text_norm]
        enthusiasm_indicators = [e for e in ENTHUSIASM_INDICATORS if e in text_norm]
        
        politeness_score = min(1.0, len(polite_phrases) / 2.0)
        empathy_score = min(1.0, len(empathy_indicators) / 1.5)
        enthusiasm_score = min(1.0, len(enthusiasm_indicators) / 1.5)
        
        # valence ê³„ì‚°
        pos_norm = min(1.0, pos_raw_score / 5.0)
        neg_norm = min(1.0, neg_raw_score / 5.0)
        valence_score = 0.4 + (pos_norm * 0.7 - neg_norm * 0.5)
        valence_score = max(0.3, min(1.0, valence_score))
        
        return WordChoiceAnalysis(
            positive_words=positive_words,
            negative_words=negative_words,
            polite_phrases=polite_phrases,
            empathy_indicators=empathy_indicators,
            enthusiasm_indicators=enthusiasm_indicators,
            politeness_score=politeness_score,
            empathy_score=empathy_score,
            enthusiasm_score=enthusiasm_score,
            valence_score=valence_score
        )
    
    def transcribe_korean(self, audio_array: np.ndarray) -> str:
        """í•œêµ­ì–´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë‹¤ì¤‘ STT ë°©ë²• ì§€ì›)"""
        if self._stt_method == "none":
            logger.warning("STT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return ""
        
        try:
            # ë°©ë²• 1: faster-whisper ì‚¬ìš©
            if self._stt_method in ["faster-whisper", "faster-whisper-tiny"] and self._asr_model is not None:
                return self._transcribe_with_faster_whisper(audio_array)
            
            # ë°©ë²• 2: OpenAI Whisper API ì‚¬ìš©
            elif self._stt_method == "openai-whisper":
                return self._transcribe_with_openai(audio_array)
            
            # ë°©ë²• 3: Google Speech-to-Text API ì‚¬ìš©
            elif self._stt_method == "google-speech":
                return self._transcribe_with_google(audio_array)
            
            else:
                logger.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” STT ë°©ë²•: {self._stt_method}")
                return ""
                
        except Exception as e:
            logger.error(f"ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def _transcribe_with_faster_whisper(self, audio_array: np.ndarray) -> str:
        """faster-whisperë¥¼ ì‚¬ìš©í•œ ì „ì‚¬"""
        try:
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            processed_audio = self._preprocess_audio(audio_array)
            
            # ì²« ë²ˆì§¸ ì‹œë„: ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            try:
                segments, _ = self._asr_model.transcribe(
                    processed_audio,
                    language="ko",
                    beam_size=1,              # 5 â†’ 1 (5ë°° ë¹ ë¦„)
                    best_of=1,               # ê¸°ë³¸ê°’ 5 â†’ 1 (5ë°° ë¹ ë¦„)
                    temperature=0.0,          # í™•ë¥ ì  ìƒ˜í”Œë§ ë¹„í™œì„±í™”
                    vad_filter=True,
                    vad_parameters=dict(
                        min_silence_duration_ms=300,  # 500 â†’ 300 (ë” ë¹ ë¥¸ ê°ì§€)
                        max_speech_duration_s=30      # ê¸´ ìŒì„± ì œí•œ
                    ),
                    condition_on_previous_text=False,
                    initial_prompt="í•œêµ­ì–´"        # í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”
                )
            except Exception as e:
                logger.warning(f"ê¸°ë³¸ ì „ì‚¬ ì‹¤íŒ¨: {e}")
                # ë‘ ë²ˆì§¸ ì‹œë„: ë‹¨ìˆœí™”ëœ ì„¤ì •
                segments, _ = self._asr_model.transcribe(
                    processed_audio,
                    language="ko",
                    beam_size=1,
                    vad_filter=False,
                    condition_on_previous_text=False
                )
            
            text_parts = [seg.text for seg in segments]
            transcript = " ".join(tp.strip() for tp in text_parts).strip()
            
            if transcript:
                logger.info(f"âœ… faster-whisper ì „ì‚¬ ì„±ê³µ: '{transcript}'")
                return transcript
            else:
                logger.warning("faster-whisper ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
                
        except Exception as e:
            if "libctranslate2" in str(e).lower():
                logger.error("âš ï¸ libctranslate2 ì˜¤ë¥˜ë¡œ faster-whisper ì „ì‚¬ ì‹¤íŒ¨")
            else:
                logger.error(f"faster-whisper ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def _transcribe_with_openai(self, audio_array: np.ndarray) -> str:
        """OpenAI Whisper APIë¥¼ ì‚¬ìš©í•œ ì „ì‚¬"""
        try:
            import tempfile
            import os
            
            # ì˜¤ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name
                torchaudio.save(temp_path, torch.tensor(audio_array).unsqueeze(0), 16000)
            
            try:
                # OpenAI API í˜¸ì¶œ (ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ë°©ì‹)
                from openai import OpenAI
                
                # í”„ë¡ì‹œ ì™„ì „ ì°¨ë‹¨ - í™˜ê²½ë³€ìˆ˜ë„ ì„ì‹œ ì •ë¦¬
                print("ğŸ”— OpenAI í´ë¼ì´ì–¸íŠ¸ ì•ˆì „ ì´ˆê¸°í™” (Voice Analyzer)")
                
                # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì „ proxy í™˜ê²½ë³€ìˆ˜ ì„ì‹œ ì œê±°
                original_env = {}
                proxy_vars = ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']
                
                for var in proxy_vars:
                    if var in os.environ:
                        original_env[var] = os.environ.pop(var)
                
                try:
                    client = OpenAI(
                        api_key=os.getenv('OPENAI_API_KEY'),
                        timeout=60.0
                    )
                    print("âœ… OpenAI ìŒì„± ë¶„ì„ ì•ˆì „ ì—°ê²° ì™„ë£Œ")
                    
                finally:
                    # í™˜ê²½ë³€ìˆ˜ ë³µì›
                    for var, value in original_env.items():
                        os.environ[var] = value
                    
                    with open(temp_path, 'rb') as audio_file:
                        response = client.audio.transcriptions.create(
                            model="whisper-1",
                            file=audio_file,
                            language="ko"
                        )
                    
                    transcript = response.text.strip()
                    if transcript:
                        logger.info(f"âœ… OpenAI Whisper ì „ì‚¬ ì„±ê³µ: '{transcript}'")
                        return transcript
                    else:
                        logger.warning("OpenAI Whisper ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                        return ""
                        
            except ImportError:
                # httpxê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                
                with open(temp_path, 'rb') as audio_file:
                    response = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        language="ko"
                    )
                
                transcript = response.text.strip()
                if transcript:
                    logger.info(f"âœ… OpenAI Whisper ì „ì‚¬ ì„±ê³µ: '{transcript}'")
                    return transcript
                else:
                    logger.warning("OpenAI Whisper ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return ""
                    
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                os.unlink(temp_path)
                
        except Exception as e:
            logger.error(f"OpenAI Whisper ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def _transcribe_with_google(self, audio_array: np.ndarray) -> str:
        """Google Speech-to-Text APIë¥¼ ì‚¬ìš©í•œ ì „ì‚¬"""
        try:
            from google.cloud import speech
            import tempfile
            import os
            
            # ì˜¤ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name
                torchaudio.save(temp_path, torch.tensor(audio_array).unsqueeze(0), 16000)
            
            try:
                # Google Speech-to-Text API í˜¸ì¶œ
                client = speech.SpeechClient()
                
                with open(temp_path, 'rb') as audio_file:
                    content = audio_file.read()
                
                audio = speech.RecognitionAudio(content=content)
                config = speech.RecognitionConfig(
                    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                    sample_rate_hertz=16000,
                    language_code="ko-KR",
                )
                
                response = client.recognize(config=config, audio=audio)
                
                transcript = ""
                for result in response.results:
                    transcript += result.alternatives[0].transcript
                
                transcript = transcript.strip()
                if transcript:
                    logger.info(f"âœ… Google Speech-to-Text ì „ì‚¬ ì„±ê³µ: '{transcript}'")
                    return transcript
                else:
                    logger.warning("Google Speech-to-Text ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return ""
                    
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                os.unlink(temp_path)
                
        except Exception as e:
            logger.error(f"Google Speech-to-Text ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def classify_emotion_audio(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:
        """ì˜¤ë””ì˜¤ì—ì„œ ì§ì ‘ ê°ì • ë¶„ë¥˜"""
        if self._audio_clf is None:
            logger.info("ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¤‘ë¦½ ê°ì • ì‚¬ìš©")
            return [("ì¤‘ë¦½", 1.0)]
        
        try:
            result = self._audio_clf(
                {"array": audio_array, "sampling_rate": TARGET_SR}, 
                top_k=None
            )
            
            # ê²°ê³¼ë¥¼ í•œêµ­ì–´ ë ˆì´ë¸”ë¡œ ë³€í™˜
            agg = {}
            for item in result:
                ko_label = AUDIO_LABEL_MAP.get(item["label"], item["label"])
                agg[ko_label] = agg.get(ko_label, 0.0) + item["score"]
            
            return sorted(agg.items(), key=lambda x: x[1], reverse=True)
            
        except Exception as e:
            logger.error(f"ìŒì„± ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return [("ì¤‘ë¦½", 1.0)]
    
    def classify_emotion_by_keywords(self, text: str) -> List[Tuple[str, float]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„"""
        if not text:
            return [("ì¤‘ë¦½", 1.0)]
        
        positive_score = sum(1 for word in POSITIVE_DATING_WORDS if word in text)
        negative_score = sum(1 for word in NEGATIVE_DATING_WORDS if word in text)
        enthusiasm_score = sum(1 for word in ENTHUSIASM_INDICATORS if word in text)
        
        if enthusiasm_score > 0 or positive_score > negative_score:
            return [("ê¸°ì¨", 0.8), ("ì¤‘ë¦½", 0.2)]
        elif negative_score > positive_score:
            return [("ìŠ¬í””", 0.7), ("ì¤‘ë¦½", 0.3)]
        else:
            return [("ì¤‘ë¦½", 1.0)]
    
    def _preprocess_audio(self, audio_array: np.ndarray) -> np.ndarray:
        """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
        audio_array = audio_array.astype(np.float32)
        
        # ë³¼ë¥¨ ì •ê·œí™”
        rms = np.sqrt(np.mean(np.square(audio_array)))
        if rms > 0:
            target_rms = 0.1
            gain = min(target_rms / rms, 10.0)
            audio_array = audio_array * gain
        
        # ê³ ì£¼íŒŒ í•„í„° (ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±°)
        try:
            from scipy import signal
            nyquist = TARGET_SR / 2
            cutoff = 80  # Hz
            b, a = signal.butter(4, cutoff / nyquist, btype='high')
            audio_array = signal.filtfilt(b, a, audio_array).astype(np.float32)
        except ImportError:
            logger.warning("scipy ì—†ìŒ - ê³ ì£¼íŒŒ í•„í„° ê±´ë„ˆëœ€")
        
        # ë…¸ì´ì¦ˆ ê²Œì´íŠ¸
        threshold = 0.01
        audio_array[np.abs(audio_array) < threshold] = 0
        
        return audio_array.astype(np.float32)
    
    def check_audio_quality(self, audio_array: np.ndarray, sr: int) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì‚¬"""
        if len(audio_array) == 0:
            return {
                "rms_level": 0.0,
                "peak_level": 0.0,
                "silence_ratio": 1.0,
                "duration": 0.0,
                "sample_count": 0,
                "is_valid": False,
                "error_message": "ë¹ˆ ì˜¤ë””ì˜¤ ë°°ì—´"
            }
        
        rms_level = float(np.sqrt(np.mean(np.square(audio_array))))
        peak_level = float(np.max(np.abs(audio_array)))
        duration = float(len(audio_array)) / float(sr)
        sample_count = len(audio_array)
        
        # ë¬´ìŒ ë¹„ìœ¨ ê³„ì‚°
        silence_threshold = 0.01
        silent_samples = np.sum(np.abs(audio_array) < silence_threshold)
        silence_ratio = silent_samples / sample_count if sample_count > 0 else 1.0
        
        # ìœ íš¨ì„± íŒë‹¨ (ì™„í™”ëœ ê¸°ì¤€)
        is_valid = True
        error_message = ""
        
        if rms_level < 0.001:  # RMS ê¸°ì¤€ ì™„í™”
            is_valid = False
            error_message = f"ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì¡°ìš©í•¨ (RMS: {rms_level:.6f})"
        elif peak_level < 0.005:  # í”¼í¬ ê¸°ì¤€ ì™„í™”
            is_valid = False
            error_message = f"ì˜¤ë””ì˜¤ í”¼í¬ê°€ ë„ˆë¬´ ë‚®ìŒ (Peak: {peak_level:.6f})"
        elif silence_ratio > 0.98:  # ë¬´ìŒ ë¹„ìœ¨ ê¸°ì¤€ ì™„í™”
            is_valid = False
            error_message = f"ë¬´ìŒì´ ë„ˆë¬´ ë§ìŒ ({silence_ratio:.1%})"
        elif duration < 0.3:  # ìµœì†Œ ê¸¸ì´ ê¸°ì¤€ ì™„í™”
            is_valid = False
            error_message = f"ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ìŒ ({duration:.2f}s)"
        
        return {
            "rms_level": rms_level,
            "peak_level": peak_level,
            "silence_ratio": silence_ratio,
            "duration": duration,
            "sample_count": sample_count,
            "is_valid": is_valid,
            "error_message": error_message
        }

    def _analyze_emotion(self, audio_array: np.ndarray, transcript: str) -> List[Tuple[str, float]]:
        """ê°ì • ë¶„ì„ (ì˜¤ë””ì˜¤ + í…ìŠ¤íŠ¸ ê²°í•©)"""
        try:
            # 1. í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì • ë¶„ì„ (ìš°ì„ )
            text_emotions = self.classify_emotion_by_keywords(transcript)
            
            # 2. ì˜¤ë””ì˜¤ ê¸°ë°˜ ê°ì • ë¶„ì„ (ë³´ì¡°)
            audio_emotions = self.classify_emotion_audio(audio_array)
            
            # 3. ê²°ê³¼ ê²°í•© (í…ìŠ¤íŠ¸ ìš°ì„ , ì˜¤ë””ì˜¤ ë³´ì¡°)
            combined_emotions = {}
            
            # í…ìŠ¤íŠ¸ ê°ì • ê°€ì¤‘ì¹˜ 0.7
            for emotion, score in text_emotions:
                combined_emotions[emotion] = combined_emotions.get(emotion, 0.0) + score * 0.7
            
            # ì˜¤ë””ì˜¤ ê°ì • ê°€ì¤‘ì¹˜ 0.3
            for emotion, score in audio_emotions:
                combined_emotions[emotion] = combined_emotions.get(emotion, 0.0) + score * 0.3
            
            # ì •ê·œí™”
            total_score = sum(combined_emotions.values())
            if total_score > 0:
                combined_emotions = {k: v / total_score for k, v in combined_emotions.items()}
            
            return sorted(combined_emotions.items(), key=lambda x: x[1], reverse=True)
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return [("ì¤‘ë¦½", 1.0)]

    def analyze_audio(
        self,
        audio_array: np.ndarray,
        sr: int = 16000,
        elapsed_sec: float = 0.0
    ) -> VoiceAnalysisResult:
        """
        ì˜¤ë””ì˜¤ë¥¼ ì™„ì „íˆ ë¶„ì„í•˜ì—¬ VoiceAnalysisResult ë°˜í™˜
        
        Args:
            audio_array: ì˜¤ë””ì˜¤ ë°ì´í„°
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            elapsed_sec: ê²½ê³¼ ì‹œê°„ (ëŒ€í™” ë§¥ë½ìš©)
            
        Returns:
            ì™„ì „í•œ ìŒì„± ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # 1. ìŒì„± ì¸ì‹ (STT)
            transcript = self.transcribe_korean(audio_array)
            if not transcript:
                transcript = "ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
            # 2. ê°ì • ë¶„ì„
            emotion_scores = self._analyze_emotion(audio_array, transcript)
            top_emotion, top_score = emotion_scores[0] if emotion_scores else ("ì¤‘ë¦½", 1.0)
            
            # 3. ìŒì„± í†¤ ë¶„ì„
            voice_tone = self.analyze_voice_tone(audio_array, sr)
            
            # 4. ë‹¨ì–´ ì„ íƒ ë¶„ì„
            word_choice = self.analyze_word_choice(transcript, elapsed_sec, voice_tone)
            
            # 5. ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì‚¬
            audio_quality = self.check_audio_quality(audio_array, sr)
            
            # 6. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # 7. VoiceAnalysisResult ìƒì„±
            result = VoiceAnalysisResult(
                transcript=transcript,
                emotion=top_emotion,
                emotion_score=top_score,
                voice_tone=voice_tone,
                word_choice=word_choice,
                total_score=0.0,  # VoiceScorerì—ì„œ ê³„ì‚°
                voice_tone_score=0.0,  # VoiceScorerì—ì„œ ê³„ì‚°
                word_choice_score=0.0,  # VoiceScorerì—ì„œ ê³„ì‚°
                overall_mood="ë³´í†µ",  # VoiceScorerì—ì„œ ê³„ì‚°
                evidence={},  # VoiceScorerì—ì„œ ìƒì„±
                recommendations=[],  # VoiceScorerì—ì„œ ìƒì„±
                processing_time=processing_time,
                audio_quality=audio_quality
            )
            
            return result
            
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            return VoiceAnalysisResult(
                transcript="ë¶„ì„ ì‹¤íŒ¨",
                emotion="ì¤‘ë¦½",
                emotion_score=1.0,
                voice_tone=VoiceToneAnalysis(0.5, 3.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5),
                word_choice=WordChoiceAnalysis([], [], [], [], [], 0.5, 0.5, 0.5, 0.5),
                total_score=50.0,
                voice_tone_score=50.0,
                word_choice_score=50.0,
                overall_mood="ë³´í†µ",
                evidence={"ì˜¤ë¥˜": "ë¶„ì„ ì‹¤íŒ¨"},
                recommendations=["ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”"],
                processing_time=time.time() - start_time,
                audio_quality={"error": "ë¶„ì„ ì‹¤íŒ¨"}
            )
